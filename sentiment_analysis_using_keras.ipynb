{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this tutorial implment sentiment analysis using keras library "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  we have implemented ANN, CNN and LSTM NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  prepare your dataset and divide into training data and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have two files in which one file contain the positive review and another file contain negetive review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os.path\n",
    "import numpy as np\n",
    "import _pickle as pic\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'      # this is used when you want to run on cpu not gpu\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_file = 'data/full_positive.txt'\n",
    "negative_file = 'data/full_negative.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we convert each vocablory in these file to their word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(file_name):\n",
    "    lexicon = []\n",
    "    with io.open(file_name, 'r', encoding='cp437') as f:\n",
    "        contents = f.readlines()\n",
    "        for l in contents:\n",
    "            all_words = tokenizer.tokenize(l)\n",
    "            #all_words = word_tokenize(l)\n",
    "            lexicon += list(all_words)\n",
    "    lexicon = [(lemmatizer.lemmatize(i)).encode('utf8') for i in lexicon]\n",
    "    lexicon = sorted(set(lexicon))\n",
    "    if not os.path.isfile(file_name+\"_vocab\"):\n",
    "        with open(file_name+\"_vocab\", \"w\") as f:\n",
    "            for word in lexicon:\n",
    "                f.write(word)\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "    return lexicon\n",
    "\n",
    "def get_unknown_vocab(fname, words):\n",
    "    f2 = io.open(fname, 'rb').read()\n",
    "    vocab = f2.splitlines()\n",
    "    known_vocab = []\n",
    "    unknown_vocab = []\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            known_vocab.append(word)\n",
    "        else:\n",
    "            unknown_vocab.append(word)\n",
    "    return known_vocab, unknown_vocab\n",
    "\n",
    "def get_wiki_glove_vector(fname, words):\n",
    "    vectors = {}\n",
    "    W = []\n",
    "    vocab = {}\n",
    "    ivocab = {}\n",
    "    f1 = open(fname, 'rb').read()\n",
    "    for line in f1.splitlines():\n",
    "        temp = line.split()\n",
    "        vectors[temp[0]] = map(float, temp[1:])\n",
    "\n",
    "    vocab_size = len(words)\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        W.append(vectors[words[i]])\n",
    "        vocab[words[i]] = i\n",
    "        ivocab[i] = words[i]\n",
    "    W = np.array(W)\n",
    "    # normalize each word vector to unit variance\n",
    "    #print W[0:2], W[-1:-3]\n",
    "    W_norm = np.zeros(W.shape)\n",
    "    d = (np.sum(W ** 2, 1) ** (0.5))\n",
    "    W_norm = (W.T / d).T\n",
    "\n",
    "    return W_norm, vocab, ivocab\n",
    "\n",
    "\n",
    "def get_unknown_vec(words, word2vec, vocab, ivocab):\n",
    "    old_size = len(vocab)\n",
    "    word2vec = list(word2vec)\n",
    "    for i in range(len(words)):\n",
    "        word2vec.append(np.random.uniform(-0.25, 0.25, len(word2vec[0])))\n",
    "        vocab[words[i]] = i+old_size\n",
    "        ivocab[i+old_size] = words[i]\n",
    "    return word2vec, vocab, ivocab\n",
    "\n",
    "glove_file = 'D:/nlp/dataset/glove.6B/glove.6B.50d.txt'\n",
    "glove_vocab_file = 'D:/nlp/dataset/glove.6B/vocab.txt'\n",
    "word2vec_file = \"data/word2vec.p\"\n",
    "\n",
    "if not os.path.isfile(word2vec_file):\n",
    "    # get vocablory of all file\n",
    "    vocab = get_vocab(positive_file)\n",
    "    vocab += get_vocab(negative_file)\n",
    "    print(\"vocab length: \", len(vocab))\n",
    "    # get the glove vector\n",
    "    known_vocab, unknown_vocab = get_unknown_vocab(glove_vocab_file, vocab)\n",
    "    print(\"known_vocab length: \", len(known_vocab))\n",
    "    print(\"unknown_vocab length: \", len(unknown_vocab))\n",
    "    Word2vec, vocab, ivocab = get_wiki_glove_vector(glove_file, known_vocab)\n",
    "    Word2vec, vocab, ivocab = get_unknown_vec(unknown_vocab,Word2vec, vocab, ivocab)\n",
    "    pic.dump([Word2vec, vocab, ivocab ], open(word2vec_file, 'wb'))\n",
    "    print(\"word2vec created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we generate sentence vector from sentence in files \n",
    "    we have two way of making sentence vector \n",
    "        first way to take average of all word2vec in sentence\n",
    "        second way to make 2D setence vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10662\n"
     ]
    }
   ],
   "source": [
    "def get_avg_sen2vec(fname, word2vec_fname, output):\n",
    "    sen2vec = []\n",
    "    with open(word2vec_fname, 'rb') as f:\n",
    "        word2vec, vocab, ivocab = pic.load(f)\n",
    "\n",
    "    with io.open(fname, 'r', encoding='cp437') as f:\n",
    "        contents = f.readlines()\n",
    "        for l in contents:\n",
    "            temp = np.zeros(len(word2vec[0]), dtype=float)\n",
    "            all_words = tokenizer.tokenize(l)\n",
    "            lexicon = list(all_words)\n",
    "            lexicon = [(lemmatizer.lemmatize(i)).encode('utf8') for i in lexicon]\n",
    "            for word in lexicon:\n",
    "                temp += word2vec[vocab[word]]\n",
    "            sen2vec.append([temp, output])\n",
    "    return sen2vec\n",
    "\n",
    "def get_2D_sen2vec(fname, word2vec_fname, output, m):\n",
    "    sen2vec = []\n",
    "    with open(word2vec_fname, 'rb') as f:\n",
    "        word2vec, vocab, ivocab = pic.load(f)\n",
    "\n",
    "    with io.open(fname, 'rb', encoding='cp437') as f:\n",
    "        contents = f.readlines()\n",
    "        for l in contents:\n",
    "            temp = np.zeros([m, len(word2vec[0])], dtype=float)\n",
    "            count = 0\n",
    "            all_words = tokenizer.tokenize(l)\n",
    "            lexicon = list(all_words)\n",
    "            lexicon = [(lemmatizer.lemmatize(i)).encode('utf8') for i in lexicon]\n",
    "            for word in lexicon:\n",
    "                temp[count] = word2vec[vocab[word]]\n",
    "                count += 1\n",
    "            sen2vec.append([temp, output])\n",
    "    return sen2vec\n",
    "\n",
    "sen2vec_file = 'data/sen2vec.p'\n",
    "\n",
    "if sen2vec_file == \"data/sen2vec.p\":\n",
    "    if not os.path.isfile(sen2vec_file):\n",
    "        # get sentence vector from file\n",
    "        sen2vec =  get_avg_sen2vec(positive_file, word2vec_file, [1,0])\n",
    "        sen2vec += get_avg_sen2vec(negative_file, word2vec_file, [0,1])\n",
    "        random.shuffle(sen2vec)\n",
    "        pic.dump(sen2vec, open(sen2vec_file, 'wb'))\n",
    "        print(\"sen2vec created\")\n",
    "\n",
    "if sen2vec_file == \"data/sen2vec2D.p\":\n",
    "    if not os.path.isfile(sen2vec_file):\n",
    "        a = dp.get_max_senLen(positive_file)\n",
    "        b = dp.get_max_senLen(negative_file)\n",
    "        m = max(a,b)\n",
    "        sen2vec = sv.get_2D_sen2vec(positive_file, word2vec_file, [1, 0], m)\n",
    "        sen2vec += sv.get_2D_sen2vec(negative_file, word2vec_file, [0, 1], m)\n",
    "        random.shuffle(sen2vec)\n",
    "        pic.dump(sen2vec, open(sen2vec_file, 'wb'))\n",
    "        print(\"sen2vec created\")\n",
    "\n",
    "# load sentence vector for training and testing\n",
    "f = io.open(sen2vec_file, 'rb')\n",
    "sen2vec = pic.load(f, encoding='latin1')\n",
    "f.close()\n",
    "print(len(sen2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make traning and testing dataset from sen2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data length:  9000\n",
      "test data length:  1662\n",
      "structure of train_x [ -7.10645047e-02   3.56886871e-01   1.51247953e+00  -4.96324830e-01\n",
      "   3.60876297e-01   3.45026011e-01  -4.64292122e-01   1.58518457e-01\n",
      "  -5.40560352e-01   5.34866654e-01  -9.02793525e-02  -9.32967806e-02\n",
      "   3.30208930e-01   1.92810272e-02   7.58572645e-02  -1.18964720e+00\n",
      "   5.21441553e-01   5.29360106e-01  -5.79081010e-01   1.99314626e-01\n",
      "   6.28924915e-01  -3.50990890e-01   2.27730109e-02   4.23964732e-03\n",
      "   6.44080553e-01  -1.70729801e-01  -2.47870729e-01  -1.04310770e+00\n",
      "   4.08848631e-01  -2.72390474e-01  -9.93259704e-01   1.68478856e+00\n",
      "  -6.85944405e-02  -4.99940288e-02  -1.26533149e-01   4.08618825e-01\n",
      "   5.77886497e-01   6.92536474e-01   2.12528441e-01  -1.19557787e-01\n",
      "  -1.30026804e+00  -3.76894191e-01   6.87990264e-01  -7.97877541e-01\n",
      "  -3.17431023e-01   3.17487425e-01   6.04146042e-01  -8.87979661e-01\n",
      "  -5.44144623e-01  -1.87603824e+00  -4.82828782e-02  -3.02600951e-01\n",
      "   7.08893668e-01   3.51561637e+00  -5.05824836e-01  -7.41546584e+00\n",
      "   2.65775848e-02  -5.91466408e-01   4.88089348e+00   1.10017021e+00\n",
      "  -1.40011792e-01   2.10132152e+00  -4.78984616e-01   3.98638582e-01\n",
      "   2.21361501e+00   2.08769129e-01   1.16652970e+00   1.05573121e+00\n",
      "   2.39400415e-01  -1.09982713e+00   2.93901831e-01  -8.59989295e-01\n",
      "  -7.85488987e-01  -1.47277908e+00   7.09153174e-01   1.70509211e-01\n",
      "  -8.46598196e-01  -3.85340594e-02  -3.11651586e+00  -3.80653940e-01\n",
      "   1.70383667e+00   1.22926552e+00  -1.05879151e+00   5.96744709e-01\n",
      "  -3.72621185e+00  -3.59261914e-01   2.77863461e-01  -4.02582133e-01\n",
      "  -2.54217135e-01  -9.70206317e-01  -1.14650783e-01   1.73718881e-01\n",
      "  -7.62432295e-01   3.11273497e-01  -1.45408659e+00  -4.18470852e-01\n",
      "  -3.85374409e-01  -1.17451830e+00   1.85396851e+00   7.21050629e-01]\n",
      "structure of train_y [1 0]\n"
     ]
    }
   ],
   "source": [
    "train_data = np.array(sen2vec[0:9000])\n",
    "test_data = np.array(sen2vec[9000:])\n",
    "train_x = np.array(list(train_data[:, 0]))\n",
    "train_y = np.array(list(train_data[:, 1]))\n",
    "test_x = np.array(list(test_data[:, 0]))\n",
    "test_y = np.array(list(test_data[:, 1]))\n",
    "print(\"train data length: \", len(train_data))\n",
    "print(\"test data length: \", len(test_data))\n",
    "print(\"structure of train_x\", train_x[0])\n",
    "print(\"structure of train_y\", train_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make feed forward neural network using keras library and train all our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9000/9000 [==============================] - 8s - loss: 0.5967 - acc: 0.6872     \n",
      "Epoch 2/10\n",
      "9000/9000 [==============================] - 6s - loss: 0.5499 - acc: 0.7173     \n",
      "Epoch 3/10\n",
      "9000/9000 [==============================] - 6s - loss: 0.5360 - acc: 0.7216     \n",
      "Epoch 4/10\n",
      "9000/9000 [==============================] - 6s - loss: 0.5274 - acc: 0.7218     \n",
      "Epoch 5/10\n",
      "9000/9000 [==============================] - 6s - loss: 0.5218 - acc: 0.7303     \n",
      "Epoch 6/10\n",
      "9000/9000 [==============================] - 6s - loss: 0.5132 - acc: 0.7377     \n",
      "Epoch 7/10\n",
      "9000/9000 [==============================] - 6s - loss: 0.5097 - acc: 0.7427     \n",
      "Epoch 8/10\n",
      "9000/9000 [==============================] - 6s - loss: 0.4996 - acc: 0.7410     \n",
      "Epoch 9/10\n",
      "9000/9000 [==============================] - 6s - loss: 0.4934 - acc: 0.7477     \n",
      "Epoch 10/10\n",
      "9000/9000 [==============================] - 6s - loss: 0.4864 - acc: 0.7522     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x91022ee828>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 100, kernel_initializer = 'uniform', activation = 'relu', input_dim = 100))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 100, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 2, kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(train_x, train_y, batch_size = 10, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we trained our ANN.\n",
    "Now time to predict the test batch to check our accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predcition:  [[ 0.0075157   0.99248427]\n",
      " [ 0.39475092  0.60524905]\n",
      " [ 0.63080257  0.36919749]\n",
      " ..., \n",
      " [ 0.99581784  0.00418222]\n",
      " [ 0.38143739  0.6185627 ]\n",
      " [ 0.4506152   0.54938477]]\n",
      "real value:  [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ..., \n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Accuracy 0.729843561974\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(test_x)\n",
    "\n",
    "print(\"predcition: \", y_pred)\n",
    "print(\"real value: \", test_y)\n",
    "\n",
    "correct = np.equal(np.argmax(y_pred, 1), np.argmax(test_y, 1))\n",
    "accuracy = np.mean(correct)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RNN LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we prepare data for rnn lstm model which use 2D sentence vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10662\n",
      "train data length:  9000\n",
      "test data length:  1662\n",
      "structure of train_x [[ 0.02354535 -0.03094732  0.00125232 ..., -0.18856544 -0.04128594\n",
      "  -0.04813067]\n",
      " [ 0.02429378 -0.09221743 -0.01113799 ..., -0.15952884 -0.13504654\n",
      "  -0.06183854]\n",
      " [ 0.07611648 -0.04697192 -0.02948809 ..., -0.16526055 -0.07888167\n",
      "   0.08149928]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "structure of train_y [1 0]\n"
     ]
    }
   ],
   "source": [
    "f = io.open('data/sen2vec2D.p', 'rb')\n",
    "sen2vec = pic.load(f, encoding='latin1')\n",
    "f.close()\n",
    "print(len(sen2vec))\n",
    "\n",
    "train_data = np.array(sen2vec[0:9000])\n",
    "test_data = np.array(sen2vec[9000:])\n",
    "train_x = np.array(list(train_data[:, 0]))\n",
    "train_y = np.array(list(train_data[:, 1]))\n",
    "test_x = np.array(list(test_data[:, 0]))\n",
    "test_y = np.array(list(test_data[:, 1]))\n",
    "print(\"train data length: \", len(train_data))\n",
    "print(\"test data length: \", len(test_data))\n",
    "print(\"structure of train_x\", train_x[0])\n",
    "print(\"structure of train_y\", train_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now make lstm model which process this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "9000/9000 [==============================] - 20s - loss: 0.2520    \n",
      "Epoch 2/2\n",
      "9000/9000 [==============================] - 19s - loss: 0.2525    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x913f0f3898>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(LSTM(units=200, activation='sigmoid', input_shape=(None, 100)))\n",
    "\n",
    "regressor.add(Dense(units = 2, activation = 'softmax'))\n",
    "\n",
    "regressor.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "regressor.fit(train_x, train_y, batch_size=100, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now predict the test data using trained lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predcition:  [[ 0.54677361  0.45322648]\n",
      " [ 0.54677367  0.45322639]\n",
      " [ 0.54677361  0.45322648]\n",
      " ..., \n",
      " [ 0.54677355  0.45322645]\n",
      " [ 0.54677361  0.45322648]\n",
      " [ 0.54677403  0.45322594]]\n",
      "real value:  [[0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " ..., \n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Accuracy 0.51504211793\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = regressor.predict(test_x)\n",
    "\n",
    "print(\"predcition: \", y_pred)\n",
    "print(\"real value: \", test_y)\n",
    "\n",
    "correct = np.equal(np.argmax(y_pred, 1), np.argmax(test_y, 1))\n",
    "accuracy = np.mean(correct)\n",
    "print(\"Accuracy\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we prepare data for cnn model \n",
    "    we use 2D sentence vector for cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 53, 100, 1) (1662, 53, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "train_x = np.reshape(train_x, [-1, 53, 100, 1])\n",
    "test_x = np.reshape(test_x, [-1, 53, 100, 1])\n",
    "print(train_x.shape, test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now make a CNN model using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "9000/9000 [==============================] - 2s - loss: 0.6339 - acc: 0.6293     \n",
      "Epoch 2/20\n",
      "9000/9000 [==============================] - 1s - loss: 0.5339 - acc: 0.7332     \n",
      "Epoch 3/20\n",
      "9000/9000 [==============================] - 1s - loss: 0.4773 - acc: 0.7670     \n",
      "Epoch 4/20\n",
      "9000/9000 [==============================] - 1s - loss: 0.4224 - acc: 0.8034     \n",
      "Epoch 5/20\n",
      "9000/9000 [==============================] - 1s - loss: 0.3626 - acc: 0.8407     \n",
      "Epoch 6/20\n",
      "9000/9000 [==============================] - 1s - loss: 0.2797 - acc: 0.8892     \n",
      "Epoch 7/20\n",
      "9000/9000 [==============================] - 1s - loss: 0.2135 - acc: 0.9207     \n",
      "Epoch 8/20\n",
      "9000/9000 [==============================] - 1s - loss: 0.1514 - acc: 0.9488     \n",
      "Epoch 9/20\n",
      "9000/9000 [==============================] - 1s - loss: 0.0998 - acc: 0.9733     \n",
      "Epoch 10/20\n",
      "9000/9000 [==============================] - 1s - loss: 0.0663 - acc: 0.9849     \n",
      "Epoch 11/20\n",
      "9000/9000 [==============================] - 1s - loss: 0.0420 - acc: 0.9926     \n",
      "Epoch 12/20\n",
      "9000/9000 [==============================] - 1s - loss: 0.0267 - acc: 0.9982     \n",
      "Epoch 13/20\n",
      "9000/9000 [==============================] - 1s - loss: 0.0149 - acc: 0.9993     \n",
      "Epoch 14/20\n",
      "9000/9000 [==============================] - 1s - loss: 0.0089 - acc: 0.9997     \n",
      "Epoch 15/20\n",
      "9000/9000 [==============================] - 1s - loss: 0.0060 - acc: 0.9999     \n",
      "Epoch 16/20\n",
      "9000/9000 [==============================] - 1s - loss: 0.0044 - acc: 1.0000     \n",
      "Epoch 17/20\n",
      "9000/9000 [==============================] - 1s - loss: 0.0031 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "9000/9000 [==============================] - 1s - loss: 0.0025 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "9000/9000 [==============================] - 1s - loss: 0.0019 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "9000/9000 [==============================] - 1s - loss: 0.0015 - acc: 1.0000     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x913f2f74a8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(40, (5, 100), input_shape = (53, 100, 1), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 1)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 2, activation = 'softmax'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(train_x, train_y, batch_size = 100, epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now predict the test batch and check the accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predcition:  [[  5.76599044e-13   1.00000000e+00]\n",
      " [  9.75252151e-01   2.47477926e-02]\n",
      " [  2.22199097e-01   7.77800918e-01]\n",
      " ..., \n",
      " [  8.40556920e-02   9.15944278e-01]\n",
      " [  3.29902917e-02   9.67009723e-01]\n",
      " [  9.99893546e-01   1.06414343e-04]]\n",
      "real value:  [[0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " ..., \n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Accuracy 0.700962695548\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(test_x)\n",
    "\n",
    "print(\"predcition: \", y_pred)\n",
    "print(\"real value: \", test_y)\n",
    "\n",
    "correct = np.equal(np.argmax(y_pred, 1), np.argmax(test_y, 1))\n",
    "accuracy = np.mean(correct)\n",
    "print(\"Accuracy\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
